{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# X_train_fpath = sys.argv[1]\n",
    "# Y_train_fpath = sys.argv[2]\n",
    "# X_test_fpath = sys.argv[3]\n",
    "# output_fpath = sys.argv[4]\n",
    "\n",
    "X_train_fpath = 'X_train'\n",
    "Y_train_fpath = 'Y_train'\n",
    "X_test_fpath = 'X_test'\n",
    "output_fpath = 'output.csv'\n",
    "\n",
    "X_train = np.genfromtxt(X_train_fpath, delimiter=',', skip_header=1)\n",
    "Y_train = np.genfromtxt(Y_train_fpath, delimiter=',', skip_header=1)\n",
    "X_test = np.genfromtxt(X_test_fpath, delimiter=',', skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# validation and shuffle(for batch)\n",
    "def _shuffle(X, y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return X[randomize], y[randomize]\n",
    "def set_validation(X, y, ratio=0.25):\n",
    "    trainlen = int(round(len(X) * (1 - ratio)))\n",
    "    return X[0:trainlen], y[0:trainlen], X[trainlen:None], y[trainlen:None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# normalization\n",
    "def normalize_0_1(X, train=True, col=None, X_min=None, X_max=None):\n",
    "    if train is True:\n",
    "        if col is None:\n",
    "            col = np.arange(X.shape[1])\n",
    "        X_max = X[:, col].max(0).reshape(1, -1)\n",
    "        X_min = X[:, col].min(0).reshape(1, -1)\n",
    "    X[:, col] = (X[:, col] - X_min) / (X_max - X_min)\n",
    "    return X, X_max, X_min\n",
    "\n",
    "def normalize_normal(X, train=True, col=None, X_mean=None, X_std=None):\n",
    "    if train is True:\n",
    "        if col is None:\n",
    "            col = np.arange(X.shape[1])\n",
    "        X_mean = X[:, col].mean(0).reshape(1, -1)\n",
    "        X_std = X[:, col].std(0).reshape(1, -1)\n",
    "    X[:, col] = (X[:, col] - X_mean) / (X_std)\n",
    "    return X, X_mean, X_std\n",
    "\n",
    "\n",
    "col = [0, 1, 3, 4, 5, 7, 10, 12, 25, 26, 27, 28]\n",
    "# col = np.arange(X_train.shape[1])[X_train.mean(0) > 1]\n",
    "X_train, X_mean, X_std = normalize_normal(X_train, col=col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "def sigmoid_(x1):\n",
    "    return np.clip(1 / (1 + np.exp(-x1)), 1e-6, 1 - 1e-6)\n",
    "\n",
    "def estimate_prob(x, w, b):\n",
    "    return np.round(sigmoid_(np.dot(x, w) + b))\n",
    "\n",
    "def cross_validation(y_pred, y_true):\n",
    "    cross_entropy = - y_true.dot(np.log(y_pred)) - (1 - y_true).dot(np.log(1 - y_pred))\n",
    "    return cross_entropy\n",
    "def gradient(x, y_true, w, b):\n",
    "    y_pred = estimate_prob(x, w, b)\n",
    "    m = len(y_true)\n",
    "    w_grad = -1 / m * ((x.T).dot(y_pred - y_true))\n",
    "    b_grad = - 1 / m * np.sum((y_pred - y_true))\n",
    "    return w_grad, b_grad\n",
    "def _gradient_regularization(X, Y_label, w, b, lamda):\n",
    "    # return the mean of the graident\n",
    "    y_pred = estimate_prob(X, w, b)\n",
    "    pred_error = Y_label - y_pred\n",
    "    w_grad = -np.mean(np.multiply(pred_error.T, X.T), 1) + lamda * w\n",
    "    b_grad = -np.mean(pred_error)\n",
    "    return w_grad, b_grad\n",
    "def regulization_gradient(x, y_true, w, b, lamba):\n",
    "    y_pred = estimate_prob(x, w, b)\n",
    "    m = len(y_true)\n",
    "    w_grad = (-1 / m) * ((x.T).dot((y_true - y_pred))) + lamba * w\n",
    "    b_grad = (- 1 / m) * np.sum(y_true - y_pred)\n",
    "    return w_grad, b_grad\n",
    "def regulization_loss(y_pred, y_true, lamba, w):\n",
    "    return cross_validation(y_pred, y_true) + lamba * np.sum(np.square(w))\n",
    "def accuracy(y_pred, y_true):\n",
    "    acc = np.sum(y_pred == y_true) / len(y_pred)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# run gradient\n",
    "def run_gradient(x_train, y_train, regulization=True, validation_ratio=0.1125, learning_rate=0.0001):\n",
    "    x_train, y_train, x_val, y_val = set_validation(X_train, Y_train, ratio=validation_ratio)\n",
    "    w = np.zeros((x_train.shape[1],))\n",
    "    b = np.zeros((1,))\n",
    "    # r_w = np.zeros((x_train.shape[1],))\n",
    "    # r_b = np.zeros((1,))\n",
    "    regulization = True\n",
    "    lamba = 0\n",
    "    if regulization is True:\n",
    "        lamba = 0.001\n",
    "    else:\n",
    "        lamba = 0\n",
    "\n",
    "    iteration = 40\n",
    "    batch_size = 32\n",
    "    # learning_rate = 0.001\n",
    "    # decay_rate = 0.99\n",
    "    step = 1  # for decay\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    # run\n",
    "    for epoch in range(iteration):\n",
    "        x_train, y_train = _shuffle(x_train, y_train)\n",
    "        for id in range(int(np.floor(len(y_train) / batch_size))):\n",
    "            X = x_train[id * batch_size:(id + 1) * batch_size]\n",
    "            Y = y_train[id * batch_size:(id + 1) * batch_size]\n",
    "\n",
    "            w_grad, b_grad = _gradient_regularization(X, Y, w, b, lamba)\n",
    "            w = w - learning_rate / np.sqrt(step) * w_grad\n",
    "            b = b - learning_rate / np.sqrt(step) * b_grad\n",
    "\n",
    "            step = step + 1\n",
    "            # r_w = r_w * decay_rate + ((1 - decay_rate) * np.multiply(w_grad, w_grad))\n",
    "            # r_b = r_b * decay_rate + ((1 - decay_rate) * np.multiply(b_grad, b_grad))\n",
    "            # w = w - ((learning_rate / (np.sqrt(r_w) + 0.0000001)) * w_grad)\n",
    "            # b = b - ((learning_rate / (np.sqrt(r_b) + 0.0000001)) * b_grad)\n",
    "\n",
    "        # loss and acc for each iteration\n",
    "        y_train_pred = estimate_prob(x_train, w, b)\n",
    "        train_acc.append(accuracy(y_train_pred, y_train))\n",
    "        loss_train.append(regulization_loss(y_train_pred, y_train, lamba, w) / len(y_train))\n",
    "\n",
    "        y_val_pred = estimate_prob(x_val, w, b)\n",
    "        val_acc.append(accuracy(y_val_pred, y_val))\n",
    "        loss_val.append(regulization_loss(y_val_pred, y_val, lamba, w) / len(y_val))\n",
    "    return w, b, train_acc, loss_train, val_acc, loss_val\n",
    "\n",
    "\n",
    "w, b, train_acc, loss_train, val_acc, loss_val = run_gradient(X_train, Y_train, regulization=True, learning_rate=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_val)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
